#  Micro Scraper API

A lightweight web scraping API built with **Node.js**, **Express**, and **Puppeteer**. 

This API allows users to scrape the `<title>`, `<meta name="description">`, and first `<h1>` element from any given webpage URL.

---

##  Features

* **GET /api/scrape?url=...** â€” Main endpoint for scraping.
* **Extracted Data:** Page **Title**, **Meta Description**, and First **H1** tag.
* **Automatic Timeout:** Cancels scraping after 20 seconds.
* **Custom User-Agent:** Simulates a real Chrome browser for reliable results.
* **Error Handling:** Provides proper HTTP status codes for errors (400, 500, 504).
* **Output:** Simple JSON format, ready for immediate integration.

---

##  Tech Stack

| Component | Description |
| :--- | :--- |
| **Node.js** | JavaScript runtime |
| **Express** | Web framework for API routing |
| **Puppeteer** | Headless browser for scraping |
| **Nodemon** | Auto-restart tool for development |

---

##  Installation & Setup

1.  **Clone this repository**
    ```bash
    git clone [https://github.com/Aul-rhmn/micro-scraper.git](https://github.com/Aul-rhmn/micro-scraper.git)
    cd micro-scraper-api
    ```

2.  **Install dependencies**
    ```bash
    npm install
    ```

3.  **Run the server**
    
    * **Development Mode (with Nodemon):**
        ```bash
        npm run dev
        ```
    * **Production Mode:**
        ```bash
        npm start
        ```

4.  The API will be accessible at:
    ```
    http://localhost:3000
    ```

---

##  API Usage

### Endpoint

````

GET /api/scrape?url=\<target\_url\>

````

### Example Request (using cURL)

```bash
curl "http://localhost:3000/api/scrape?url=[https://example.com](https://example.com)"
````

### Example Response (Success)

```json
{
  "title": "Example Domain",
  "metaDescription": "This domain is for use in illustrative examples in documents.",
  "h1": "Example Domain",
  "status": 200
}
```

-----

##  Error Responses

| Status Code | Description | Example Response |
| :---: | :--- | :--- |
| **400** | Invalid or missing URL parameter. | `{"error": "Invalid URL"}` |
| **504** | Scraping timeout (page took longer than 20 seconds). | `{"error": "Timeout"}` |
| **500** | Internal server or scraping failure (e.g., DNS error, connection issue). | `{"error": "Scraping failed (e.g., DNS or connection issue)"}` |

-----

##  How It Works Internally

1.  The API validates the input URL parameter.
2.  It launches a **headless Chromium** browser instance via Puppeteer.
3.  The browser waits until the target page is fully loaded.
4.  It executes JavaScript to extract the required elements: `<title>`, `<meta name="description">` content, and the text of the first `<h1>` tag.
5.  If successful, the results are returned as a JSON object.
6.  A **20-second timer** ensures the process is terminated and returns a 504 status code if the page takes too long to respond.

-----

##  Available Scripts

| Command | Description |
| :--- | :--- |
| `npm start` | Runs the server in production environment. |
| `npm run dev` | Runs the server in development mode using Nodemon (auto-restarts on file changes). |

-----

##  License

This project is licensed.

```
```
